rbac:
  create: false

alertmanager:
  ## If false, alertmanager will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## alertmanager container name
  ##
  name: alertmanager

  ## alertmanager container image
  ##
  image:
    repository: prom/alertmanager
    tag: v0.20.0
    pullPolicy: IfNotPresent

  ## Additional alertmanager container arguments
  ##
  extraArgs:
  #  log.level: debug

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  baseURL: "http://reddit-alertmanager"

  ## Additional alertmanager container environment variable
  ## For instance to add a http_proxy
  ##
  extraEnv: {}

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, alertmanager Ingress will be created
    ##
    enabled: true

    ## alertmanager Ingress annotations
    ##
    annotations:
      kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## alertmanager Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts:
      - reddit-alertmanager

    ## alertmanager Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - alertmanager.domain.com

  # Alertmanager Deployment Strategy type
  strategy:
    type: Recreate

  ## Node labels for alertmanager pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  persistentVolume:
    ## If true, alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: false

    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## alertmanager data Persistent Volume mount root path
    ##
    mountPath: /data

    ## alertmanager data Persistent Volume size
    ##
    size: 2Gi

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## Subdirectory of alertmanager data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to alertmanager pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## alertmanager resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the alertmanager service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    # nodePort: 30000
    #type: ClusterIP
    #type: LoadBalancer
    type: NodePort

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/jimmidyson/configmap-reload
##
configmapReload:
  ## configmap-reload container name
  ##
  name: configmap-reload

  ## configmap-reload container image
  ##
  image:
    repository: jimmidyson/configmap-reload
    tag: v0.1
    pullPolicy: IfNotPresent

  ## configmap-reload resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

kubeStateMetrics:
  ## If false, kube-state-metrics will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## kube-state-metrics container name
  ##
  name: kube-state-metrics

  ## kube-state-metrics container image
  ##
  image:
    repository: gcr.io/google_containers/kube-state-metrics
    tag: v1.1.0
    pullPolicy: IfNotPresent

  ## Node labels for kube-state-metrics pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to kube-state-metrics pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## kube-state-metrics resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    {}
    # limits:
    #   cpu: 10m
    #   memory: 16Mi
    # requests:
    #   cpu: 10m
    #   memory: 16Mi

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    clusterIP: None

    ## List of IP addresses at which the kube-state-metrics service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

nodeExporter:
  ## If false, node-exporter will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## node-exporter container name
  ##
  name: node-exporter

  ## node-exporter container image
  ##
  image:
    repository: prom/node-exporter
    tag: v0.15.1
    pullPolicy: IfNotPresent

  ## Custom Update Strategy
  ##
  updateStrategy:
    type: OnDelete

  ## Additional node-exporter container arguments
  ##
  extraArgs: {}

  ## Additional node-exporter hostPath mounts
  ##
  extraHostPathMounts:
    []
    # - name: textfile-dir
    #   mountPath: /srv/txt_collector
    #   hostPath: /var/lib/node-exporter
    #   readOnly: true

  ## Node tolerations for node-exporter scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations:
    []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for node-exporter pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to node-exporter pods
  ##
  podAnnotations: {}

  ## node-exporter resource limits & requests
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    {}
    # limits:
    #   cpu: 200m
    #   memory: 50Mi
    # requests:
    #   cpu: 100m
    #   memory: 30Mi

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    clusterIP: None

    ## List of IP addresses at which the node-exporter service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    hostPort: 9100
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9100
    type: ClusterIP

server:
  ## Prometheus server container name
  ##
  name: server

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## Prometheus server container image
  ##
  image:
    repository: prom/prometheus
    tag: v2.0.0
    pullPolicy: IfNotPresent

  ## (optional) alertmanager hostname
  ## only used if alertmanager.enabled = false
  alertmanagerHostname: ""

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  baseURL: ""

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Additional Prometheus server hostPath mounts
  ##
  extraHostPathMounts:
    []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/server-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: true

    ## Prometheus server Ingress annotations
    ##
    annotations:
      kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Prometheus server Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts:
      - reddit-prometheus

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-server-tls
    #     hosts:
    #       - prometheus.domain.com

  ## Server Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations:
    []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for Prometheus server pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 8Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## Subdirectory of Prometheus server data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to Prometheus server pods
  ##
  podAnnotations:
    {}
    # iam.amazonaws.com/role: prometheus

  replicaCount: 1

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    {}
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    # requests:
    #   cpu: 500m
    #   memory: 512Mi

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    #type: LoadBalancer
    type: NodePort

  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (i.e 360h)
  ##
  retention: ""

pushgateway:
  ## If false, pushgateway will not be installed
  ##
  enabled: false

  ## pushgateway container name
  ##
  name: pushgateway

  ## pushgateway container image
  ##
  image:
    repository: prom/pushgateway
    tag: v0.4.0
    pullPolicy: IfNotPresent

  ## Additional pushgateway container arguments
  ##
  extraArgs: {}

  ingress:
    ## If true, pushgateway Ingress will be created
    ##
    enabled: false

    ## pushgateway Ingress annotations
    ##
    annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## pushgateway Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - pushgateway.domain.com

    ## pushgateway Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - pushgateway.domain.com

  ## Node labels for pushgateway pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to pushgateway pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## pushgateway resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  service:
    annotations:
      prometheus.io/probe: pushgateway
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the pushgateway service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9091
    type: ClusterIP

## alertmanager ConfigMap entries
##
alertmanagerFiles:
  alertmanager.yml:
    receivers:
      - name: default-receiver
        slack_configs:
          - api_url: "https://hooks.slack.com/services/hereisasecret"
            channel: "#vscoder-k5"
            send_resolved: true

    route:
      group_wait: 10s
      group_interval: 5m
      receiver: default-receiver
      # repeat_interval: 3h
      repeat_interval: 5m

## Prometheus server ConfigMap entries
##
serverFiles:
  alerts: {}
  rules: {}

  ## Alerts configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  ## alerting_rules.yml: {}
  alerting_rules.yml:
    groups:
      - name: Instances
        rules:
          - alert: InstanceDown
            expr: up == 0
            for: 30s
            labels:
              severity: page
            annotations:
              description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."
              summary: "Instance {{ $labels.instance }} down"
      - "name": "kubernetes-apps"
        "rules":
          - "alert": "KubePodCrashLooping"
            "annotations":
              "message": 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.'
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping"
            "expr": |
              rate(kube_pod_container_status_restarts_total{component="kube-state-metrics"}[15m]) * 60 * 5 > 0
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubePodNotReady"
            "annotations":
              "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready"
            "expr": |
              sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{component="kube-state-metrics", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeDeploymentGenerationMismatch"
            "annotations":
              "message": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch"
            "expr": |
              kube_deployment_status_observed_generation{component="kube-state-metrics"}
                !=
              kube_deployment_metadata_generation{component="kube-state-metrics"}
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeDeploymentReplicasMismatch"
            "annotations":
              "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch"
            "expr": |
              kube_deployment_spec_replicas{component="kube-state-metrics"}
                !=
              kube_deployment_status_replicas_available{component="kube-state-metrics"}
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeStatefulSetReplicasMismatch"
            "annotations":
              "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch"
            "expr": |
              kube_statefulset_status_replicas_ready{component="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas{component="kube-state-metrics"}
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeStatefulSetGenerationMismatch"
            "annotations":
              "message": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch"
            "expr": |
              kube_statefulset_status_observed_generation{component="kube-state-metrics"}
                !=
              kube_statefulset_metadata_generation{component="kube-state-metrics"}
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeStatefulSetUpdateNotRolledOut"
            "annotations":
              "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout"
            "expr": |
              max without (revision) (
                kube_statefulset_status_current_revision{component="kube-state-metrics"}
                  unless
                kube_statefulset_status_update_revision{component="kube-state-metrics"}
              )
                *
              (
                kube_statefulset_replicas{component="kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas_updated{component="kube-state-metrics"}
              )
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeDaemonSetRolloutStuck"
            "annotations":
              "message": "Only {{ $value | humanizePercentage }} of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck"
            "expr": |
              kube_daemonset_status_number_ready{component="kube-state-metrics"}
                /
              kube_daemonset_status_desired_number_scheduled{component="kube-state-metrics"} < 1.00
            "for": "15m"
            "labels":
              "severity": "critical"
          - "alert": "KubeContainerWaiting"
            "annotations":
              "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}} has been in waiting state for longer than 1 hour."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting"
            "expr": |
              sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{component="kube-state-metrics"}) > 0
            "for": "1h"
            "labels":
              "severity": "warning"
          - "alert": "KubeDaemonSetNotScheduled"
            "annotations":
              "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled"
            "expr": |
              kube_daemonset_status_desired_number_scheduled{component="kube-state-metrics"}
                -
              kube_daemonset_status_current_number_scheduled{component="kube-state-metrics"} > 0
            "for": "10m"
            "labels":
              "severity": "warning"
          - "alert": "KubeDaemonSetMisScheduled"
            "annotations":
              "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled"
            "expr": |
              kube_daemonset_status_number_misscheduled{component="kube-state-metrics"} > 0
            "for": "10m"
            "labels":
              "severity": "warning"
      - "name": "kubernetes-resources"
        "rules":
          - "alert": "KubeCPUOvercommit"
            "annotations":
              "message": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
            "expr": |
              sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
                /
              sum(kube_node_status_allocatable_cpu_cores)
                >
              (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
            "for": "5m"
            "labels":
              "severity": "warning"
          - "alert": "KubeMemOvercommit"
            "annotations":
              "message": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit"
            "expr": |
              sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
                /
              sum(kube_node_status_allocatable_memory_bytes)
                >
              (count(kube_node_status_allocatable_memory_bytes)-1)
                /
              count(kube_node_status_allocatable_memory_bytes)
            "for": "5m"
            "labels":
              "severity": "warning"
          - "alert": "KubeCPUOvercommit"
            "annotations":
              "message": "Cluster has overcommitted CPU resource requests for Namespaces."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
            "expr": |
              sum(kube_resourcequota{component="kube-state-metrics", type="hard", resource="cpu"})
                /
              sum(kube_node_status_allocatable_cpu_cores)
                > 1.5
            "for": "5m"
            "labels":
              "severity": "warning"
          - "alert": "KubeMemOvercommit"
            "annotations":
              "message": "Cluster has overcommitted memory resource requests for Namespaces."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit"
            "expr": |
              sum(kube_resourcequota{component="kube-state-metrics", type="hard", resource="memory"})
                /
              sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
                > 1.5
            "for": "5m"
            "labels":
              "severity": "warning"
          - "alert": "KubeQuotaExceeded"
            "annotations":
              "message": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded"
            "expr": |
              kube_resourcequota{component="kube-state-metrics", type="used"}
                / ignoring(instance, job, type)
              (kube_resourcequota{component="kube-state-metrics", type="hard"} > 0)
                > 0.90
            "for": "15m"
            "labels":
              "severity": "warning"
          - "alert": "CPUThrottlingHigh"
            "annotations":
              "message": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"
            "expr": |
              sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
                /
              sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
                > ( 25 / 100 )
            "for": "15m"
            "labels":
              "severity": "warning"
      - "name": "kubernetes-storage"
        "rules":
          - "alert": "KubePersistentVolumeUsageCritical"
            "annotations":
              "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical"
            "expr": |
              kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubernetes-nodes"}
                < 0.03
            "for": "1m"
            "labels":
              "severity": "critical"
          - "alert": "KubePersistentVolumeFullInFourDays"
            "annotations":
              "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays"
            "expr": |
              (
                kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}
                  /
                kubelet_volume_stats_capacity_bytes{job="kubernetes-nodes"}
              ) < 0.15
              and
              predict_linear(kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}[6h], 4 * 24 * 3600) < 0
            "for": "1h"
            "labels":
              "severity": "critical"
          - "alert": "KubePersistentVolumeErrors"
            "annotations":
              "message": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors"
            "expr": |
              kube_persistentvolume_status_phase{phase=~"Failed|Pending",component="kube-state-metrics"} > 0
            "for": "5m"
            "labels":
              "severity": "critical"
      - "name": "kubernetes-system"
        "rules":
          - "alert": "KubeVersionMismatch"
            "annotations":
              "message": "There are {{ $value }} different semantic versions of Kubernetes components running."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch"
            "expr": |
              count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
            "for": "15m"
            "labels":
              "severity": "warning"
          - "alert": "KubeClientErrors"
            "annotations":
              "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'"
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors"
            "expr": |
              (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
                /
              sum(rate(rest_client_requests_total[5m])) by (instance, job))
              > 0.01
            "for": "15m"
            "labels":
              "severity": "warning"
      - "name": "kubernetes-apiservers-error"
        "rules":
          - "alert": "ErrorBudgetBurn"
            "annotations":
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn"
            "expr": |
              (
                status_class_5xx:apiserver_request_total:ratio_rate1h{job="kubernetes-apiservers"} > (14.4*0.010000)
                and
                status_class_5xx:apiserver_request_total:ratio_rate5m{job="kubernetes-apiservers"} > (14.4*0.010000)
              )
              or
              (
                status_class_5xx:apiserver_request_total:ratio_rate6h{job="kubernetes-apiservers"} > (6*0.010000)
                and
                status_class_5xx:apiserver_request_total:ratio_rate30m{job="kubernetes-apiservers"} > (6*0.010000)
              )
            "labels":
              "job": "kubernetes-apiservers"
              "severity": "critical"
          - "alert": "ErrorBudgetBurn"
            "annotations":
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn"
            "expr": |
              (
                status_class_5xx:apiserver_request_total:ratio_rate1d{job="kubernetes-apiservers"} > (3*0.010000)
                and
                status_class_5xx:apiserver_request_total:ratio_rate2h{job="kubernetes-apiservers"} > (3*0.010000)
              )
              or
              (
                status_class_5xx:apiserver_request_total:ratio_rate3d{job="kubernetes-apiservers"} > (0.010000)
                and
                status_class_5xx:apiserver_request_total:ratio_rate6h{job="kubernetes-apiservers"} > (0.010000)
              )
            "labels":
              "job": "kubernetes-apiservers"
              "severity": "warning"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[5m]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate5m"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[30m]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate30m"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[1h]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate1h"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[2h]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate2h"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[6h]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate6h"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[1d]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate1d"
          - "expr": |
              sum by (status_class) (
                label_replace(
                  rate(apiserver_request_total{job="kubernetes-apiservers"}[3d]
                ), "status_class", "${1}xx", "code", "([0-9])..")
              )
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class:apiserver_request_total:rate3d"
          - "expr": |
              sum(status_class:apiserver_request_total:rate5m{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate5m{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate5m"
          - "expr": |
              sum(status_class:apiserver_request_total:rate30m{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate30m{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate30m"
          - "expr": |
              sum(status_class:apiserver_request_total:rate1h{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate1h{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate1h"
          - "expr": |
              sum(status_class:apiserver_request_total:rate2h{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate2h{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate2h"
          - "expr": |
              sum(status_class:apiserver_request_total:rate6h{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate6h{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate6h"
          - "expr": |
              sum(status_class:apiserver_request_total:rate1d{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate1d{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate1d"
          - "expr": |
              sum(status_class:apiserver_request_total:rate3d{job="kubernetes-apiservers",status_class="5xx"})
              /
              sum(status_class:apiserver_request_total:rate3d{job="kubernetes-apiservers"})
            "labels":
              "job": "kubernetes-apiservers"
            "record": "status_class_5xx:apiserver_request_total:ratio_rate3d"
      - "name": "kubernetes-system-apiserver"
        "rules":
          - "alert": "KubeAPILatencyHigh"
            "annotations":
              "message": "The API server has an abnormal latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh"
            "expr": |
              (
                cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"}
                >
                on (verb) group_left()
                (
                  avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"} >= 0)
                  +
                  2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"} >= 0)
                )
              ) > on (verb) group_left()
              1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"} >= 0)
              and on (verb,resource)
              cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kubernetes-apiservers",quantile="0.99"}
              >
              1
            "for": "5m"
            "labels":
              "severity": "warning"
          - "alert": "KubeAPILatencyHigh"
            "annotations":
              "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh"
            "expr": |
              cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kubernetes-apiservers",quantile="0.99"} > 4
            "for": "10m"
            "labels":
              "severity": "critical"
          - "alert": "KubeAPIErrorsHigh"
            "annotations":
              "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
            "expr": |
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) > 0.03
            "for": "10m"
            "labels":
              "severity": "critical"
          - "alert": "KubeAPIErrorsHigh"
            "annotations":
              "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
            "expr": |
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) > 0.01
            "for": "10m"
            "labels":
              "severity": "warning"
          - "alert": "KubeAPIErrorsHigh"
            "annotations":
              "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
            "expr": |
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m])) by (resource,subresource,verb)
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) by (resource,subresource,verb) > 0.10
            "for": "10m"
            "labels":
              "severity": "critical"
          - "alert": "KubeAPIErrorsHigh"
            "annotations":
              "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
            "expr": |
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m])) by (resource,subresource,verb)
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) by (resource,subresource,verb) > 0.05
            "for": "10m"
            "labels":
              "severity": "warning"
          - "alert": "KubeClientCertificateExpiration"
            "annotations":
              "message": "A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration"
            "expr": |
              apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m]))) < 604800
            "labels":
              "severity": "warning"
          - "alert": "KubeClientCertificateExpiration"
            "annotations":
              "message": "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration"
            "expr": |
              apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m]))) < 86400
            "labels":
              "severity": "critical"
          - "alert": "KubeAPIDown"
            "annotations":
              "message": "KubeAPI has disappeared from Prometheus target discovery."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown"
            "expr": |
              absent(up{job="kubernetes-apiservers"} == 1)
            "for": "15m"
            "labels":
              "severity": "critical"
      - "name": "kubernetes-system-kubelet"
        "rules":
          - "alert": "KubeNodeNotReady"
            "annotations":
              "message": "{{ $labels.node }} has been unready for more than 15 minutes."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready"
            "expr": |
              kube_node_status_condition{component="kube-state-metrics",condition="Ready",status="true"} == 0
            "for": "15m"
            "labels":
              "severity": "warning"
          - "alert": "KubeNodeUnreachable"
            "annotations":
              "message": "{{ $labels.node }} is unreachable and some workloads may be rescheduled."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable"
            "expr": |
              kube_node_spec_taint{component="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
            "for": "2m"
            "labels":
              "severity": "warning"
          - "alert": "KubeletTooManyPods"
            "annotations":
              "message": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity."
              "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods"
            "expr": |
              max(max(kubelet_running_pod_count{job="kubernetes-nodes"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job="kubernetes-nodes"}) by(node) / max(kube_node_status_capacity_pods{component="kube-state-metrics"}) by(node) > 0.95
            "for": "15m"
            "labels":
              "severity": "warning"

  ## Records configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
  recording_rules.yml: {}

  prometheus.yml:
    rule_files:
      - /etc/config/recording_rules.yml
      - /etc/config/alerting_rules.yml
      # - /etc/config/rules
      # - /etc/config/alerts

    global:
      scrape_interval: 30s

    scrape_configs:
      - job_name: "comment-endpoints"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app]
            action: keep
            regex: reddit
          - source_labels: [__meta_kubernetes_service_label_component]
            action: keep
            regex: comment
          - action: labelmap # Отобразить все совпадения групп
            regex: __meta_kubernetes_service_label_(.+) # из regex в label’ы Prometheus
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      - job_name: "ui-endpoints"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app]
            action: keep
            regex: reddit
          - source_labels: [__meta_kubernetes_service_label_component]
            action: keep
            regex: ui
          - action: labelmap # Отобразить все совпадения групп
            regex: __meta_kubernetes_service_label_(.+) # из regex в label’ы Prometheus
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      - job_name: "post-endpoints"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app]
            action: keep
            regex: reddit
          - source_labels: [__meta_kubernetes_service_label_component]
            action: keep
            regex: post
          - action: labelmap # Отобразить все совпадения групп
            regex: __meta_kubernetes_service_label_(.+) # из regex в label’ы Prometheus
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      - job_name: "reddit-production"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels:
              [__meta_kubernetes_service_label_app, __meta_kubernetes_namespace]
            action: keep
            regex: reddit;(production|staging)+
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      - job_name: prometheus
        static_configs:
          - targets:
              - localhost:9090

      # A scrape configuration for running Prometheus on a Kubernetes cluster.
      # This uses separate scrape configs for cluster components (i.e. API server, node)
      # and services to allow each to use different authentication configs.
      #
      # Kubernetes labels will be added as Prometheus labels on metrics via the
      # `labelmap` relabeling action.

      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: "kubernetes-apiservers"

        kubernetes_sd_configs:
          - role: endpoints

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels:
              [
                __meta_kubernetes_namespace,
                __meta_kubernetes_service_name,
                __meta_kubernetes_endpoint_port_name,
              ]
            action: keep
            regex: default;kubernetes;https

      - job_name: "kubernetes-nodes"

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: "kubernetes-service-endpoints"

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [
                __address__,
                __meta_kubernetes_service_annotation_prometheus_io_port,
              ]
            action: replace
            target_label: __address__
            regex: (.+)(?::\d+);(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name

      - job_name: "prometheus-pushgateway"
        honor_labels: true

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: pushgateway

      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: "kubernetes-services"

        metrics_path: /probe
        params:
          module: [http_2xx]

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: "kubernetes-pods"

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels:
              [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: (.+):(?:\d+);(\d+)
            replacement: ${1}:${2}
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false
